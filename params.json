{"name":"Wine-Quality-Prediction","tagline":"This project uses Wine Dataset from UC Irvine Machine Learning Repository to predict whether a particular red wine is of  “good quality” or not. Each wine in this dataset is given a “quality” score between 0 and 10.We use decision tree model to predict the quality of  wine.","body":"## **WINE QUALITY PREDICTION** ##\r\n **I love everything that’s old — old friends, old times, old manners, old books, old wine. — Oliver Goldsmith**\r\n \r\n ![](https://miro.medium.com/max/1050/1*2ayKmvVZCYaLPl-nmLLp5g.png)\r\n \r\n## **INTRODUCTION** ##\r\n\r\nMost of us love wine. A good wine always makes the occasion better. Its always the quality of the wine that matters . Lets apply machine learning model to figure  out what makes a qood quality wine.\r\nFor this project , I have used the Wine Dataset from UC Irvine Machine Learning Repository.It consists 11 input variables to predict the quality of wine .\r\n\r\n**1.Fixed acidity**\r\n\r\n**2.Volatile acidity**\r\n\r\n**3.Citric acid**\r\n\r\n**4.Residual sugar**\r\n\r\n**5.Chlorides**\r\n\r\n**6.Free sulfur dioxide**\r\n\r\n**7.Total sulfur dioxide**\r\n\r\n**8.Density**\r\n\r\n**9.pH**\r\n\r\n**10.Sulfates**\r\n\r\n**11.Alcohol**\r\n\r\nWe can predict the quality of wine using different models which yields different results. Here we use **Scikit-learn’s Decision Tree Classifier**.Decision trees are intuitive and easy to build . Let's learn more about Decision trees.\r\n\r\n# **Decision Tree** #\r\n![Decision tree](https://miro.medium.com/max/945/1*f_tt4OIzuY4yoPrnFP0gdA.png)\r\n\r\nDecision trees are a popular model, used in operations research, strategic planning and machine learning. Each square above is called a node, and the more nodes you have, the more accurate your decision tree will be (generally). The last nodes of the decision tree, where a decision is made, are called the leaves of the tree.\r\n\r\nThe root node (the first decision node) partitions the data based on the most influential feature partitioning. There are 2 measures for this, Gini Impurity and Entropy.\r\n\r\n**Entropy**\r\n\r\nThe root node (the first decision node) partitions the data using the feature that provides the most information gain.\r\nInformation gain tells us how important a given attribute of the feature vectors is.\r\nIt is calculated as:\r\n\r\nInformation Gain=entropy(parent)–[average entropy(children)]\r\nWhere entropy is a common measure of target class impurity, given as:\r\n\r\nEntropy=Σi–pilog2pi\r\nwhere i is each of the target classes.\r\n\r\n**Gini Impurity**\r\n\r\nGini Impurity is another measure of impurity and is calculated as follows:\r\n\r\nGini=1–Σip2i\r\n\r\nGini impurity is computationally faster as it doesn’t require calculating logarithmic functions, though in reality which of the two methods is used rarely makes too much of a difference.\r\n\r\n![Classification tree of red wine before pruning](https://cdn-images-1.medium.com/max/800/0*zT39Hb7MzF5rMt7b)\r\n\r\n\r\n\r\n**Now let's predict the quality of wine**\r\n\r\n## Importing libraries ##\r\n\r\nFirst let's import all the required libraries\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport seaborn as sns\r\nimport matplotlib\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn import preprocessing\r\nfrom sklearn import tree\r\nfrom sklearn.metrics import classification_report\r\n```\r\n\r\nnumpy will be used for making the mathematical calculations more accurate, pandas will be used to work with file formats like csv, xls etc. and sklearn (scikit-learn) will be used to import our classifier for prediction. Seaborn provides a high-level interface for drawing attractive and informative statistical graphics.It is a Python data visualization library based on matplotlib.\r\nMatplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy.\r\n\r\nfrom sklearn.model_selection import train_test_split is used to split our dataset into training and testing data, more of which will be covered later. The next import, from sklearn import preprocessing is used to preprocess the data before fitting into predictor, or converting it to a range of (-1,1), which is easy to understand for the machine learning algorithms. Next import, from sklearn import tree is used to import our decision tree classifier, which we will be using for prediction.\r\nA Classification report is used to measure the quality of predictions .\r\n\r\n## **Reading data** ##\r\n\r\nThe very next step is importing the data that  we will be using. For this project, we will be using the Wine Dataset from UC Irvine Machine Learning Repository.\r\n```\r\ndata=pd.read_csv('winedataset.csv')\r\n```\r\n\r\nWe use pd.read_csv() function in pandas to import the data by giving the name of the dataset . We use ‘;’ (semi-colon) as  separator to obtain the csv in a more structured format.\r\n\r\nNow we have to analyse the dataset. First we will see what is inside the data set by seeing the first five values of dataset by head() command.\r\n```\r\ndata.head()\r\n```\r\n\r\n```\r\n  fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\r\n0            7.4              0.70         0.00             1.9      0.076   \r\n1            7.8              0.88         0.00             2.6      0.098   \r\n2            7.8              0.76         0.04             2.3      0.092   \r\n3           11.2              0.28         0.56             1.9      0.075   \r\n4            7.4              0.70         0.00             1.9      0.076   \r\n\r\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\r\n0                 11.0                  34.0   0.9978  3.51       0.56   \r\n1                 25.0                  67.0   0.9968  3.20       0.68   \r\n2                 15.0                  54.0   0.9970  3.26       0.65   \r\n3                 17.0                  60.0   0.9980  3.16       0.58   \r\n4                 11.0                  34.0   0.9978  3.51       0.56   \r\n\r\n   alcohol  quality  \r\n0      9.4        5  \r\n1      9.8        5  \r\n2      9.8        5  \r\n3      9.8        6  \r\n4      9.4        5 \r\n```\r\nhead() Information Of Wine Dataset\r\n\r\n## **Analysing the data**  ##\r\nNow we need to analyse the data . First lets see the distribution of quality variable. Lets make the histogram of quality variable.\r\n\r\n```\r\nx=data.quality\r\nfig=plt.hist(x,bins=10)\r\n```\r\n![Quality histogram](https://miro.medium.com/max/1050/1*mby2pZfP2mnyi5wP_Uh9sQ.png)\r\n\r\n\r\nNext lets see the correlation between variables. This will help us  to understand the relationship between variables.\r\n\r\n```\r\ncorr = data.corr()\r\nmatplotlib.pyplot.subplots(figsize=(15,10))\r\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))\r\n```\r\n![correlation graph](https://miro.medium.com/max/1050/1*dhDaVItzF4dzxLlJ8Kyhlw.png)\r\n\r\nWe can observe that there are certain variables that  are strongly correlated to the *quality*. Its most likely that these features are very important for determining the quality.\r\n\r\n## **Convert to a Classification Problem**  ##\r\n\r\nEvery machine learning model has two things **Features** and **Labels** . Features are the part of a dataset which are used to predict the label. Labels  on the other hand are mapped to features. After the model has been trained, we give features to it, so that it can predict the labels.\r\n\r\nSo in this dataset, we need to find the quality of the wine. So quality is our label and and all other columns are features. We need to separate features and label into two different data frames.\r\n\r\n```\r\ny=data.quality\r\nX=data.drop('quality',axis=1)\r\n```\r\nHere we stored *quality* in y and all other attributes in X.\r\n\r\n## **Split data**  ##\r\n\r\nNext we need to split our dataset into train and test data sets . Train data set is used to train the model and test data set is used to test the created model.This splitting is done by train_test_split() function.\r\n\r\n```\r\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1)\r\n```\r\n_Splitting Into Test And Train Data_\r\n\r\n\r\nWe have used test_size=0.1 which means 10% of the original data is used for testing and remaining 90% of data is used for training the model.\r\nNow lets print the first five elements of data we have split using head() function.\r\n\r\n```\r\nX_train.head()\r\n```\r\n\r\n```\r\n      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\r\n591             6.6              0.39         0.49             1.7      0.070   \r\n1196            7.9              0.58         0.23             2.3      0.076   \r\n1128           10.0              0.43         0.33             2.7      0.095   \r\n640             9.9              0.54         0.45             2.3      0.071   \r\n389             9.6              0.38         0.31             2.5      0.096   \r\n\r\n      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\r\n591                  23.0                 149.0  0.99220  3.12       0.50   \r\n1196                 23.0                  94.0  0.99686  3.21       0.58   \r\n1128                 28.0                  89.0  0.99840  3.22       0.68   \r\n640                  16.0                  40.0  0.99910  3.39       0.62   \r\n389                  16.0                  49.0  0.99820  3.19       0.70   \r\n\r\n      alcohol  \r\n591      11.5  \r\n1196      9.5  \r\n1128     10.0  \r\n640       9.4  \r\n389      10.0  \r\n```\r\n_Training Data Using head()_\r\n\r\n\r\n## **Data normalization**  ##\r\n\r\nWe have now obtained the data we want to use. Once we have got the data we need to normailze the data. Normalization is the pre-processing in which data is converted to fit in a range of -1 and 1.\r\n\r\n```\r\nX_train_scaled = preprocessing.scale(X_train)\r\nprint X_train_scaled\r\n```\r\n_Train Data Preprocessing_\r\n\r\n\r\nAfter pre-processing\r\n```\r\narray([[-0.87446643,  0.79860489, -0.6229427 , ...,  0.70432307,\r\n         0.06387655,  0.82832417],\r\n       [-1.10298858,  0.79860489, -0.98056848, ...,  1.15526353,\r\n        -0.98197072,  0.92191388],\r\n       [ 1.41075502,  2.0455708 ,  0.14339824, ..., -2.06573976,\r\n         3.02711047, -0.20116258],\r\n       ...,\r\n       [-0.64594429,  0.28848248,  1.36954375, ...,  0.12454248,\r\n        -0.51714971, -0.10757288],\r\n       [ 1.86779932, -0.67508208,  1.36954375, ..., -1.87247956,\r\n         0.58680018, -0.4819317 ],\r\n       [ 0.61092752, -0.73176235,  0.19448764, ..., -1.55037923,\r\n        -0.16853395,  0.07960653]])\r\n```\r\n\r\nWe can observe that all the data are within the range -1 to 1.\r\n\r\n## **Modelling** ##\r\n\r\nNow lets train our algorithm so that it can predict the quality of wine. We can do it by importing DecisionTreeClassifier() and we need to use fit() to train it.\r\n\r\n```\r\nclassifier=tree.DecisionTreeClassifier()\r\nclassifier.fit(X_train,y_train)\r\n\r\n```\r\nNow lets check the efficiency of algorithm in predicting the quality of wine. We use score() function to check the accuracy.\r\n\r\n```\r\nefficiency=classifier.score(X_test,y_test)\r\nprint('Efficiency score',efficiency)\r\n```\r\n\r\n```\r\nEfficiency score 0.69375\r\n```\r\nAs we we can see the efficiency of our algorithm in finding the quality is 0.69375\r\nThis score can change over time depending on the size of your dataset and shuffling of data when we divide the data into test and train, but you can always expect a range of ±5 around your first result.\r\n\r\n\r\nNow lets predict the data. We have trained our classifier with features, so let's obtain the labels using predict() function.\r\n\r\n```\r\nprediction=classifier.predict(X_test)\r\nprediction\r\n```\r\n\r\n```\r\narray([6, 6, 5, 6, 5, 6, 5, 5, 6, 6, 8, 5, 5, 6, 6, 5, 7, 5, 6, 7, 6, 5,\r\n       6, 6, 6, 5, 6, 5, 6, 5, 5, 7, 5, 5, 7, 6, 5, 6, 6, 7, 6, 5, 6, 6,\r\n       7, 6, 5, 5, 6, 5, 6, 7, 7, 5, 5, 5, 6, 5, 6, 6, 5, 7, 5, 7, 5, 6,\r\n       6, 5, 6, 7, 5, 5, 6, 6, 5, 5, 5, 6, 5, 7, 6, 6, 5, 6, 7, 5, 5, 5,\r\n       7, 6, 6, 6, 6, 6, 5, 6, 5, 7, 5, 4, 7, 6, 6, 4, 6, 6, 5, 6, 6, 5,\r\n       5, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 7, 6, 7, 7, 6, 5, 5, 5, 5, 4, 5,\r\n       6, 5, 6, 5, 6, 7, 5, 6, 5, 6, 7, 5, 7, 6, 6, 5, 6, 7, 7, 6, 5, 6,\r\n       5, 4, 6, 7, 5, 4], dtype=int64\r\n\r\n```\r\nOur predicted information is in prediction but it has many columns to comapre with the expected labels we stored in y_test.So we need to take first five entries of prediction and compare it with y_test.\r\n\r\n```\r\nx=np.array(prediction).tolist()\r\nprint('The prediction:\\n')\r\nfor i in range (0,5):\r\n    print(x[i])\r\nprint('The expectation:\\n')\r\ny_test.head()\r\n```\r\n_Comparing The Predicted And Expected Labels_\r\n\r\n\r\nWe converted the numpy array into list so that we can compare easily.Then we printed the first five elements of that list using for loop. Now we print the five values that we were expecting, which were stored in y_test using head() function. The output looks something like this\r\n\r\n```\r\nThe prediction:\r\n\r\n6\r\n5\r\n5\r\n5\r\n5\r\n\r\nThe expectation:\r\n\r\nOut[121]:\r\n3       6\r\n1262    5\r\n1391    5\r\n1222    6\r\n758     5\r\nName: quality, dtype: int64\r\n```\r\n_The Output_\r\n\r\n\r\nWe can notice that almost all of the values in the prediction are similar to the expectations. Our predictor got wrong just once, predicting 4th entry 6 as 5 but that’s it. This gives us the accuracy of 80% for 5 examples. Of course, as the examples increases the accuracy goes down, precisely to 0.621875 or 62.1875%, but overall our predictor performs quite well, in-fact any accuracy % greater than 50% is considered as great.\r\n\r\nWe can also print the classification report to the above problem by importing classification_report from sklearn.metrics and can find the average accuracy .\r\n\r\n```\r\nprint(classification_report(y_test,prediction))\r\n```\r\n\r\n```\r\n        precision    recall  f1-score   support\r\n\r\n           3       0.00      0.00      0.00         0\r\n           4       0.00      0.00      0.00         2\r\n           5       0.81      0.70      0.75        80\r\n           6       0.59      0.73      0.66        56\r\n           7       0.70      0.64      0.67        22\r\n\r\n    accuracy                           0.69       160\r\n   macro avg       0.42      0.41      0.41       160\r\nweighted avg       0.71      0.69      0.70       160\r\n\r\n```\r\n_Classification report_\r\n\r\n**Thus we predict the quality of wine using Decison tree model.**\r\n\r\n## **Thank you!!**  ##\r\n\r\nAssignment during Online internship with DLithe(www.dlithe.com)\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}